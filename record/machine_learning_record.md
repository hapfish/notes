
# machine_learning_record.md  


# 基础概念  

## R语言核心技术手册  

``` 
第20章  回归的机器学习算法

  回归树模型  
  MARS算法  
  神经网络  
  投影寻踪回归  
  广义可加模型  
  支持向量机  

 
第21章 分类模型  机器学习分类模型  
  k邻近   
  分类树模型  
  神经网络  
  支持向量机  
  随机森林  


第22章  机器学习  
  聚类 距离度量 聚类算法    
 
```  

 


-- CART
构建树模型最受欢迎的一种方法是分类回归树, 或者成为CART. CART对训练集使用贪心算法来构建树. 

1. 使用下面的(递归)方法构建树  
    A. 从一个包含所有训练数据的单一集合开始  
    B. 如果观测值的数目小于划分所要求的最小值, 则停止树的划分. 将训练集中y值的平均值作为该终端节点的预测值  
    C. 找到变量xj和值s使得将数据分城两个集合时的RMS误差最小  
    D. 对分好的两个数据集重复划分的过程(从B步开始)  	

2. 使用下面的(迭代)方法对树进行剪枝  
    A. 如果树只有一个节点则停止  
    B. 度量整个树的成本/复杂性. (成本/复杂性标准是一种同时考虑每个节点的观察值个数, RMS预测误差以及树的节点个数的度量方式.)	 
	C. 对树中每个节点尝试剪枝, 并记录具有最好的成本/复杂性的子树.  
	D. 对具有最好的成本/复杂性的子树重复整个过程(从A步开始).  
	
3. 输出最低成本/复杂性的树 
	
	
-- MARS	
另一种很受欢迎的机器学习算法是多元自适应回归样条, 即MARS. MARS把输入变量分成很多基函数(basis function), 然后对这些基函数拟合线性回归模型. MARS使用的基函数是成对的: f(x) = {x-t 若x>t, 否则为0} 和 g(x) = {t-x 若x<t, 否则为0}. 这些函数是分段线性(piecewise linear)函数. t值称为结点(knot).  
MARS和CART密切相关. 和CART类似, 它也从构建一个大的模型开始, 然后不断修剪掉不需要的项目, 直到最终找到最好的模型. MARS算法用基函数逐步建立模型, 指导达到了预先设定的深度. 这会导致过拟合, 过度复杂的模型. 然后该算法从模型中一个接一个地删除项目, 直到修建了除常数项之外的所有项目. 在每一个阶段, 该算法都使用广义交叉检验(GCV)来度量每个模型拟合的好坏. 最后, MARS返回具有最佳成本收益比的模型.  

-- 神经网络  
神经网络是一种非常受欢迎的统计模型. 其最初的设计目的是模拟神经在人类大脑中的工作. 很多最初的关于神经网络的研究都来自于人工智能的研究人员. 神经网络非常灵活而且可以用来对很多复杂问题进行建模. 通过改变神经网络的结构, 可以对一些非常复杂的非线性关系建模. 所以, 神经网络相当受欢迎, 甚至有专门研究它的学术期刊(比如Elsevier出版的Neural Networks).  


-- 投影寻踪回归  
投影寻踪回归是展现非线性关系的另一种很通用的模型. 
函数gm称为岭函数(ridge function). 投影寻踪算法通过最小化残差平方和的方法尝试得到参数wm的最优化结果.  
投影寻踪和神经网络模型有着密切的联系(注意, 二者方程的形式相似). 如果使用sigmoid函数来表示岭函数gm, 投影寻踪将会和神经网络相同.  


-- 广义可加模型  
广义可加模型是对高维数据集中复杂关系建模的另一种回归模型技术.  
注意, 每个预测变量xj首先经过函数fj的处理, 然后用户线性模型. 广义可加模型要找出函数f的形式. 这些函数通常称为基函数.  


-- SVM  
支持向量机(SVM)适用于非线性模型的一种非常新的算法. 对于非数学专业的人而言, 要解释SVM比解释大多数的统计模型算法困难得多. 详细解释清楚SVM如何工作已经超出了本书的范围, 这里只提供一个简单的概述. 

SVM并不依赖所有的潜在数据来训练模型. 只有一些观察值(称为支持向量)被使用. 这使得SVM在做回归时能在一定程度上防止异常值的问题(类似稳健回归的技术). 也可以以一种相反的方式使用SVM, 如侦查数据中的异常现象. 可以通过不敏感损失函数参数epsilon来控制值的范围.  

SVM使用输入数据的非线性转换(类似加性模型中的基函数或者核方法中的核). 可以通过kernel参数来控制SVM用到的核的类型.  

最终的SVM模型使用最大似然估计的标准回归方法来拟合.  

SVM是一种黑盒模型. 通过查看拟合的SVM模型的参数是很难获悉任何信息的. 但是, SVM越来越受欢迎, 很多人发现SVM在真实世界的情况下表现得很好. (Oracle数据挖掘软件中有SVM, 但很多其他的算法都没有包含在该软件中.)  




-- k邻近  
k邻近(k nearest neighbors) 是解决分类问题的一种最简单的算法. 该算法的原理为:  
  1. 分析师构建训练的数据集.  
  2. 为了预测新数据的标签, 算法首先在训练数据集中寻找与新数据距离最近的k个观测值.  
  3. 新数据的标签标记为这k个邻居的投票结果.  


-- 分类树模型  
分类树(classification tree)模型与回归树几乎一样. 二者主要有两个关键区别. 第一, 分类回归树用不同的误差函数来度量不同的分割训练数据方式, 具体来说, 即使用Gini系数来度量模型的复杂度. 第二, 分类回归树用不同的方法来选择预测值. 叶子结点的预测值是通过测试数据的响应变量值投票来选取的.    

-- 神经网络  
神经网络可以解决回归和分类问题.  

-- 支持向量机  
SVM同样能够处理分类和回归的问题.  

-- 随机森林  
随机森林是另外一种解决回归或者分类问题的算法.  




-- 聚类  
数据挖掘技术中还有一个重要的算法是聚类. 聚类是从数据集中挖掘相似观测值集合的方法. 一组相似的观测值称为簇(cluster). R中有许多函数可以实现聚类.  


距离度量  
为了有效利用聚类算法, 首先需要度量观测值间的距离. 在R中常通过stats包里面的dist函数来实现. 
dist函数计算对象(矩阵或数据框)中两两间的距离, 返回的是距离矩阵(dist类对象). 

聚类算法  
k-means聚类是最简单的聚类算法之一. R中可以通过stats包里面的kmeans函数实现k-means聚类.  


 


## R语言与网站分析  

```  

第5章  时间序列分析  
  时间序列  
  增长率  
  移动平均  
  指数平滑  
  ARIMA模型  
  

第6章  连续指标建模:回归分析  
  一元线性回归  
  多元回归分析  
  Logic回归分析  
  回归树CART  
  


第7章  分类指标建模:分类分析  
  决策树分类分析  
  贝叶斯分类  
  支持向量机SVM  
  人工神经网络  
  分类器的性能评估  

  
第8章  样本细分  
  数据降维  
  聚类分析  
  样本判别 knn(k最近邻分类) 基于knn算法的商品推荐系统    
  



```  

  
----- 第5章  

时间序列对象是一种专为时间序列分析而设计的对象类型, 其中不仅包含指标的数值, 还包括时间轴信息.  
在R语言中, ts(<向量对象>)可以把一个向量转化为一个时间序列对象.  

环比增长率主要用于描述指标data的变化趋势, 其定义式为:  
ring.growth[t] = (data[t] - data[t-1]) / data[t-1]  

如果想避免周期性变化给数据分析带来的影响, 可以使用同比增长率来进行分析. 其定义格式如下:  
same.per.growth[t] = (date[t] - data(t-T)) / data(t-T)  
其中T是同比周期, 即data[t]和data[t-T]分别是前后两个周期中对应的指标数据.  


移动平均是从"信号与分析系统学"中发展出来的一种典型滤波方法. 其思想是使用近期的样本数据来预测当前和未来的数据. 它可以把抖动的曲线平滑化, 从而弱化随机变化和短期周期性变化的影响, 进而更明显地显示出长期的变化趋势曲线.  
简单/加权移动平均虽然可以过滤微小的随机变化, 突出长期变化趋势. 但是如果使用它进行预测, 其对指标实际的变化(周期性或长期趋势下的变化)并不敏感, 整体上会做相应的平移, 而这会导致预测值的滞后性.  


指数平滑预测的数值不仅可以体现指标的长期变化趋势trend, 还能体现其周期性seasonality变化趋势. 根据平滑次数的不同, 指数平滑又分为一次指数平滑, 二次指数平滑和三次指数平滑. 其中, 一次指数平滑可以建立平滑数据s[t], 并结合使用实际数据x[t]对指标x进行预测.   二次指数平滑通过对平滑数据s[t]的差分数据 s[t]-s[t-1] 进行再次平滑, 建立二次平滑数据b[t], 并结合一次平滑的预测值来对指标x进行预测, 其中b[t] 可以代表指标x的长期趋势. 三次指数平滑在二次指数平滑的基础上对 x[t]-s[t] 或 x[t]/s[t] 进行三次指数平滑,  并建立平滑数据p[t], 然后结合二次平滑的预测值来对指标x进行预测, 其中p[t]可以代表指标x的周期性变化趋势.   

ARIMA模型是时下较为流行的时间序列分析模型, 它通过对白噪声的组合来建立模型, 并保证模型和实际数据的残差也是一个均值很小的正态分布数据.   



----- 第6章   

数据指标的建模指的是, 使用若干自变量并建立公式, 以预测目标变量(因变量).  
如果研究的目标变量是连续型的, 则称其为回归分析(本章); 如果是分类型, 则称其为分类分析(下一章).  

回归分析的最基础的情况: 一元线性回归分析. 它规定模型f函数只能是 y = kx + b 的形式, 即只使用一个变量x(故称为一元)的线性形式来预测目标变量y.  
一元线性回归分析的原理  
  1. 最小二乘法  
  2. 样本方差和协方差  
  3. 计算模型的参数k和b  
  4. 衡量相关程度  
  5. 回归系数的显著性检验  
     T检验, F检验    
  6. 模型误差(残差)   
  7. 预测  
  
多元线性回归分析建模与一元回归分析类似.   


理论上, 回归分析是在目标变量为连续型数据的情况下建模的, 它并不能处理目标变量为分类型数据的情况. 而login回归分析的思路是吧 "是否问题" 转化为 "是的概率" (连续变量), 进而使用回归分析的方法间接地研究分类分析的问题.    

回归分析是使用最小二乘法(旨在使获得的模型能最大限度地拟合样本数据)来估算模型参数的, login分析则使用最大似然法来估算. 其基本思想是: 当从总体中随机抽取n个样本后, 最合理的参数估计量应该使得这n个样本观测值的概率最大.    

回归树(也称分类回归树CART)主要以一种树状结构来表达回归分析模型的回归算法, 该类方法不仅可以应用于回归分析(称为回归树), 也可用于分类分析(称为分类树).   
R语言实现CART算法的核心函数是rpart包的epart函数.   
把继续做分支的节点叫做决策节点(non-leaf node), 不再进行分支的节点叫叶节点(leaf-node).  
一般来说, 随着拆分的增多(nsplit增加), 复杂性参量(cp)会单调下降(纯度越来越高), 但是预测误差(xerror)则会先降后升, 一般允许误差在一个标准差(xstd)内波动. 这就产生了在剪枝理论中比较著名的规则: 1-标准差规则. 其意思是: 首先要保证预测误差尽量小, 但不一定要去最小值, 而是允许它在 "最小的误差 +- 一个相应标准差" 的范围内.   




----- 第7章  

决策树算法的输出通常类似于一套由if-else(二分支)或case(多分支)语句组成的判定式, 因其画出的形状很像一棵树, 故把此类算法统称为决策树算法, 常用的决策树算法有C4.5, CART等.   
决策树的建立过程实际就是节点的分裂(split)过程.  
由于存在过分拟合等问题, 所以往往要对模型进行剪枝. 剪枝处理后, 虽然训练集的错误率会有所增加, 但是却能让新样本的预测错误率下降.  
  前剪枝preprune: 在模型建立过程中就完成剪枝, 即加强停止分裂的条件.   
  后剪枝postprune: 在模型建立之后才进行剪枝. 在剪枝前要设置停止剪枝的相应参数, 然后从模型的叶节点逐层向上剪枝.   
  
ID3算法是分类分析中的鼻祖级算法, 它以信息论中的信息增益为核心思想来建立树模型. C4.5算法实际上就是在ID3算法的基础上优化而成的.   
C4.5算法在另一个数据挖掘开源软件weka已经有很成熟的实现, 在R语言中, 可以通过RWeka包中的J48函数来调用weka的C4.5算法.   

CART的全称是Classification And Regression Tree, 即分类回归树. 它即可以对分类型目标变量建模(分类分析), 也可以对连续型目标变量建模(回归分析).  
与C4.5算法类似, CART算法也使用目标变量的纯度来分裂决策节点, 只是它使用的分裂度量是Gini增益. 需要注意的是, CART内部只支持二分支树.  

随机森林(random forest)算法是一种专门为决策树分类器设计的优化算法. 它综合了多颗决策树模型的预测结果, 其中的每棵树都是基于随机样本的一个独立集合的值产生的. 首先基于固定概率分布, 从原始训练集中可重复地选取N个样本形成t个子训练集.   然后使用这t个训练集产生t棵决策树. 最后把这t棵决策树综合组成一棵决策树.   


贝叶斯分类器(Bayesian classifiers) 是基于贝叶斯定理计算目标变量条件概率P{Y|X}(自变量在不同取值下, 目标变量各分类取值的概率)的一种建模方法. 在R语言中, 贝叶斯统计分析以及分类器的包很丰富, 这里主要介绍朴素贝叶斯算法的实现方式: e1071包中的naiveBayes函数.   
朴素贝叶斯算法不仅可以处理分类型自变量, 还可以处理连续型变量, 其原理这里暂不介绍.  

支持向量机SVM  
  1. 超平面  
  2. 超平面的边缘  
  3. 最大边缘超平面  

如果考虑使用直线作为超平面, 则称其为线性SVM, 而如果使用曲线作为超平面, 则称其为非线性SVM.   
在R语言中使用e1071包的svm函数来实现非线性svm分析.  


神经网络起源于生物学中有关神经元的研究. 神经元通过控制自身的神经突触接受信息. 这些神经突触的连接又被认为是大脑行为的关键因素. 所谓人工神经网络, 就是使用一套函数模型模拟这些相互连接的神经元. 它即可以对连续型目标变量做回归分析, 也可对分类型目标变量做分类分析.  
神经元是神经网络中的基本单元, 主要分为权重系数, 加法器, 激活函数3个部分.  
下面是针对上面3个部分的说明.  
  带有权重系数wj的突触, 连接到输入值xj, 其中 j = 1, 2, ...m.  
  加法器将所有的输入做加权. 并与一个偏差bias求和, 记为v. 可以把偏差bias看作是输入x0和权重恒为1的系数w0之积, 即 bias = w0 * x0 .  
  激活函数g(也称为压缩函数), 并把g(v)作为神经元中的输出, 该函数必须是单调函数.  

人工神经网络可分为输入层, 隐藏层和输出层3个部分. 假设要使用3个自变量对1个三元分类变量(取值为A, B, C)进行两层神经网络分析, 并把隐藏层中的神经元数设为2, 那么这3个层面实现的形式如下:  
  输入层: 连接神经网络的所有输入变量, 它并未做任何数据处理, 只负责把每个输入变量与隐藏层中的神经元连接起来.  
  隐藏层: 包含两个神经元, 并把隐藏层中神经元的输出作为每个输出层中神经元的输入. 一般来说两层网络(即一个隐藏层, 一个输出层)就已经很充分了. 但有时三层, 四层或五层的网络结构(把隐藏层中的层级数逐步增大)会更有效.  
  输出层: 对于二元分类分析问题来说, 输出层往往只有一个神经元, 输出结果为0还是1由输出值的取舍点分开. 在多元分类问题中, 输出层中神经元数的最大值为目标变量的取值分类数.  

反向传播算法(backpropagetion) 是神经网络中的经典算法, 其基本原理如下.  
  1) 设置神经网络的基本结构和初始参数, 这里主要是设置  
    隐藏层的层数和内部的神经元数目  
    隐藏层和输出层内所有神经元的加权系数及偏差量  
    所有神经元使用的激活函数  
  2) 把样本1中的自变量数据通过输入层正向输入至初始网络模型ANN(init)中, 并计算神经网络的输出y(out)(正向传播).  
  3) 计算样本1中目标变量与y(out)的误差, 并通过输出层反向输入初始网络模型中. 通过计算误差的传播来调整各个神经元中加权系数和偏差量的取值(反向传播), 最后得到由样本1修正后的神经网络模型ANN(1).  
  4) 重复上面两个步骤, 逐次使用训练集中的样本数据修整神经网络模型中的参数, 最后根据停止学习条件, 停止模型的修改过程, 并完成算法的建立. 注意, 训练集数据可以被重复学习, 不必由于所有训练集均被遍历而停止学习.  

正向传播处理  
这里以两层神经网络为例, 其隐藏层的层级数为1. 在做正向传播处理时, 首先遍历隐藏层的所有神经元, 并计算其输出. 其中w.hidden代表隐藏层中所有神经元内加权系数的矩阵(矩阵的行数等于神经元数, 列数表示自变量数). bias,hidden 代表神经元偏差量的向量(因为每个神经元内只有1个偏差量, 所以隐藏层内所有神经元的偏差量使用向量表示即可). ann.cell是神经元的处理函数.  
遍历输出层的所有神经元, 计算其输出o.out.  
w.out和bias.out分别表示输出层内神经元的加权系数和偏差量.   

反向传播处理  
输出层的误差error.out  
隐藏层的误差error.hidden  

停止学习条件   
停止学习条件一般分为3种情况(任意一个条件成立, 则停止学习)  
  1) 每次使用一个样本进行学习时, 所有神经元内的加权系数差 均小于指定的阈值  
  2) 被错误分类的样本占总样本数的比例小于指定的阈值  
  3) 执行了指定次数的处理循环  
  
ann函数来实现一层隐藏层神经网络模型. 在R语言中, nnet包的nnet函数也可以建立一层隐藏层神经网络模型, 对任意闭区间的连续函数均可以使用这样的一种神经网络来逼近实际结果.  
在nnet中可使用参数size来设置隐藏层中神经元的个数.  



分类器的性能评估  
1) 混淆矩阵  
混淆矩阵(Confusion Matrix)用于把样本实际值(true class)和模型预测值(predicted class)进行关联列表分析. 如果实际样本true class取 -1 则为反例 (negatice),  取 +1则为正例(positive). 如果模型预测predicted class错误则为假(false), 预测正确则为真(true).  
正确率和错误率  
查准率和查全率  
查全率也叫灵敏度和召回率.  

2) ROC曲线和AUC  
灵敏度(sensitivity) 正确分类的正例数(TP)占实际正例(P=TP+FN)的比例, 即 Sensitivity=TPR=TP/(TP+FN)  
特指度(specicity) 正确分类的反例数(TN)占实际反例(N=TN+FP)的比例, 即 Specicity=TNR=TN/(TN+FP)   1-Specicity=FPR=FP/(TN+FP)  
依次把灵敏度和 1-特指度绘制于平面中, 即可得到ROC曲线.  

如果ROC曲线经过点(0, 1), 即特指度=1, 灵敏度=1, 则表示为最优的分类器, 然而绝大多数的ROC曲线并非如此, 此时可通过引入ROC曲线下的面积AUC来衡量不同模型间ROC曲线的表现情况. AUC的面积越大, 该模型的ROC曲线表现越好, 模型越可用.  

3) 提升度和提升曲线  
提升度lift的定义是: lift=TPR/((TP+FP)/C), 即提升度lift等于TPR初一预测正例的比例. 这里把 (TP+FP)/C称为深度depth.  
提升曲线是以深度为横轴, 以提升度为纵轴绘制的曲线.  

4)  洛仑兹曲线  
洛仑兹(lorenz)曲线和提升曲线类似, 只是将提升曲线的纵轴由lift(=TPR/depth)改为TPR, 也就是说, 该曲线的纵轴是TPRm 横轴是深度depth=(TP+FP)/C.  

如果是类似信用评分的问题, 希望尽可能完全地识别出那些有风险的客户(如预测是否为机器人登录问题), 并不使一人漏网, 则需要考虑尽量增大TPR(查全率), 同时减小FPR(减少误判), 因此选择ROC曲线及相应的AUC作为指标.  
如果是类似精确营销的问题, 希望通过对全体消费者进行分类, 从而得到具有较高响应率的客户群(如预测用户购买新书问题), 以便提高投入产出比, 则需要考虑尽量提高lift(提升度), 同时depth不能太小(如果只给一个消费者发放宣传信息, 虽然响应率比较大, 却无法得到足够多的响应), 因此选择提升曲线或洛仑兹曲线作为指标.  


----- 第8章 样本细分  

数据降维主要有主成分分析和因子分析两种方式. 可以认为主成分分析是一种特殊情况下的因子分析.  

在R语言中, factanal函数采用主流的最大似然法来估算因子载荷A, 从而实现因子分析过程.  

聚类分析是典型的非监督分析, 其分析的目的并不是对某一目标变量建模, 而是对距离相近的样本进行归类(Q型聚类)或对相近的变量指标进行归类(R型聚类).  

1) 连续变量距离  

常用距离说明:  
- 绝对值距离  
- 欧式距离  
- 闵可夫斯基距离  
- 切比雪夫距离  
- Lance距离  
- 归一化距离  

在R语言中, 使用dist函数可以把一个矩阵或数据框转化为距离矩阵.  


对于连续性变量的距离计算, 实际存在一个量纲问题. 所谓量纲, 就是指标的单位.  

2) 分类型变量距离  
有序分类变量的取值只有固定值(类似于分类型变量), 但是其数值又有大小含义(类似于连续型变量).  
一个二元分类变量的取值仅为0或1.  
无序分类变量可以把每个p元变量转化为p个非对称二元变量, 并使用相同的样本数来计算距离.  

以上距离只适用于所有变量均是连续型或均是分类型的情况. 对于混合型的样本数据, 可以考虑分别计算连续型和分类型的距离, 最后再汇成综合距离.  

3) 混合变量处理    
上面介绍的连续型变量, 有序分类变量, 对称二元分类变量, 非对称二元分类变量, 以及无序分类变量的距离计算只适用于所有变量局势以上类型的情况. 但实际数据库中, 数据对象往往是用复合数据类型来描述的, 而且它们(同时)常常包含上述几种数据类型. 一个更好的处理方法就是将所有类型的变量距离全部映射到[0, 1]范围内, 然后把它们放在一起进行综合叠加计算.   
dis.factor()函数是自定义的用于计算无序分类变量以及二元变量距离的函数. 如果参数blance设置为FALSE, 则计算非对称二院变量的距离, 否则计算无序分类变量以及对称二元变量的距离.  

4) 相似度距离   
前面介绍的距离实际是以样本数据的差异为基础来衡量的, 相似度距离则是以样本数据的相似程度来衡量的. 相似度距离还有一个典型的应用: 通过计算变量间的相似度距离来对变量进行聚类, 即完成R型聚类.  
使用相似度描述的距离常常用于计算两个变量间的距离, 并完成R型聚类.  


层次聚类通过分解给定的数据集来创建一个层次. 根据层次分解形成的方式, 可以将层次方法分为自下而上和自上而下两种类型, 有些类似于决策树的生长和停止生长.  
自下而上: 从每个样本均为一个(单独的)组开始, 逐步将这些样本组合并, 指导合并到层次顶端或满足终止条件为止, 此方法也称为聚合.  
自上而下: 从所有样本均属于一个组开始, 每一次循环都会将样本分解为更小的组, 直到每个组都由一个样本构成或满足终止条件为止, 此方法也称为分解.  

在R语言中, 使用hclust函数进行层次聚类, 其基本形式是hclust(d, method = "complete").  

kmeans聚类算法是根据聚类组中的均值进行聚类划分的方法, 相比于层次聚类, 其处理大数据的性能更好, 且收敛速度更快. Kmeans算法的输入是聚类数组k, 以及包含n个样本的数据. 输出是满足方差最小标准的k个聚类.  
  1) 确定初始质心点  
  2) 根据质心点对各个样本进行分类  
  3) 重新计算各个分类内的质心点  
在R语言中, 使用kmeans函数来进行kmeans算法计算.  



knn(k最近邻分类)算法  
假设训练数据中, 每个样本点都含有变量y, 那么y值刚好就是该样本点的所属组别. 例如, 如果有两个类, 那么y就是一个二元的变量. Knn(k-最近相邻) 方法指的是在训练数据集中动态确定与新样本点(case[new])距离最相近(根据自变量Xi来计算距离)的k个样本点(case[1], case[2], ... case[k]), 并利用这k个样本点的所属组别(y[1], y[2], ..., y[k])来确定新样本点所属组别(y[new])的方法.  

注意对参数k的确定:  
  如果 k = 1, 即 1-NN, 则新样本点的所属组别y[new]就是最近样本的所属组别y[1]. 可以证明, 如果有大量的数据及充分复杂的分类规则, 则该方法在减少误差率方面, 最多能减到用1-NN算法规则时的一半.  
  如果增加k值, 则可以提供一个平滑的分类, 以便减少训练数据中噪声样本的影响. 在典型的应用中, k的取值为个位数或十位数, 而不是百位或千位数.  
  如果k值很高并接近于训练集样本数, 则仅预测在训练数据集中大多数样本所属的组别, 而不用管新样本的值如何, 这是一个过渡平滑的例子.  
  
在R语言中, 使用class包的knn函数可以实现knn算法的基本功能.  

  
  
 








 


  
  
	
  


  
  

 
  







  










  