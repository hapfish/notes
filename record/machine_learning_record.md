
# machine_learning_record.md  


# 基础概念  

## R语言核心技术手册  

``` 
第20章  回归的机器学习算法

  回归树模型  
  MARS算法  
  神经网络  
  投影寻踪回归  
  广义可加模型  
  支持向量机  

 
第21章 分类模型  机器学习分类模型  
  k邻近   
  分类树模型  
  神经网络  
  支持向量机  
  随机森林  


第22章  机器学习  
  聚类 距离度量 聚类算法    
 
```  

 

```  
-- CART
构建树模型最受欢迎的一种方法是分类回归树, 或者成为CART. CART对训练集使用贪心算法来构建树. 

1. 使用下面的(递归)方法构建树  
    A. 从一个包含所有训练数据的单一集合开始  
    B. 如果观测值的数目小于划分所要求的最小值, 则停止树的划分. 将训练集中y值的平均值作为该终端节点的预测值  
    C. 找到变量xj和值s使得将数据分城两个集合时的RMS误差最小  
    D. 对分好的两个数据集重复划分的过程(从B步开始)  	

2. 使用下面的(迭代)方法对树进行剪枝  
    A. 如果树只有一个节点则停止  
    B. 度量整个树的成本/复杂性. (成本/复杂性标准是一种同时考虑每个节点的观察值个数, RMS预测误差以及树的节点个数的度量方式.)	 
	C. 对树中每个节点尝试剪枝, 并记录具有最好的成本/复杂性的子树.  
	D. 对具有最好的成本/复杂性的子树重复整个过程(从A步开始).  
	
3. 输出最低成本/复杂性的树 
	
	
-- MARS	
另一种很受欢迎的机器学习算法是多元自适应回归样条, 即MARS. MARS把输入变量分成很多基函数(basis function), 然后对这些基函数拟合线性回归模型. MARS使用的基函数是成对的: f(x) = {x-t 若x>t, 否则为0} 和 g(x) = {t-x 若x<t, 否则为0}. 这些函数是分段线性(piecewise linear)函数. t值称为结点(knot).  
MARS和CART密切相关. 和CART类似, 它也从构建一个大的模型开始, 然后不断修剪掉不需要的项目, 直到最终找到最好的模型. MARS算法用基函数逐步建立模型, 指导达到了预先设定的深度. 这会导致过拟合, 过度复杂的模型. 然后该算法从模型中一个接一个地删除项目, 直到修建了除常数项之外的所有项目. 在每一个阶段, 该算法都使用广义交叉检验(GCV)来度量每个模型拟合的好坏. 最后, MARS返回具有最佳成本收益比的模型.  

-- 神经网络  
神经网络是一种非常受欢迎的统计模型. 其最初的设计目的是模拟神经在人类大脑中的工作. 很多最初的关于神经网络的研究都来自于人工智能的研究人员. 神经网络非常灵活而且可以用来对很多复杂问题进行建模. 通过改变神经网络的结构, 可以对一些非常复杂的非线性关系建模. 所以, 神经网络相当受欢迎, 甚至有专门研究它的学术期刊(比如Elsevier出版的Neural Networks).  


-- 投影寻踪回归  
投影寻踪回归是展现非线性关系的另一种很通用的模型. 
函数gm称为岭函数(ridge function). 投影寻踪算法通过最小化残差平方和的方法尝试得到参数wm的最优化结果.  
投影寻踪和神经网络模型有着密切的联系(注意, 二者方程的形式相似). 如果使用sigmoid函数来表示岭函数gm, 投影寻踪将会和神经网络相同.  


-- 广义可加模型  
广义可加模型是对高维数据集中复杂关系建模的另一种回归模型技术.  
注意, 每个预测变量xj首先经过函数fj的处理, 然后用户线性模型. 广义可加模型要找出函数f的形式. 这些函数通常称为基函数.  


-- SVM  
支持向量机(SVM)适用于非线性模型的一种非常新的算法. 对于非数学专业的人而言, 要解释SVM比解释大多数的统计模型算法困难得多. 详细解释清楚SVM如何工作已经超出了本书的范围, 这里只提供一个简单的概述. 

SVM并不依赖所有的潜在数据来训练模型. 只有一些观察值(称为支持向量)被使用. 这使得SVM在做回归时能在一定程度上防止异常值的问题(类似稳健回归的技术). 也可以以一种相反的方式使用SVM, 如侦查数据中的异常现象. 可以通过不敏感损失函数参数epsilon来控制值的范围.  

SVM使用输入数据的非线性转换(类似加性模型中的基函数或者核方法中的核). 可以通过kernel参数来控制SVM用到的核的类型.  

最终的SVM模型使用最大似然估计的标准回归方法来拟合.  

SVM是一种黑盒模型. 通过查看拟合的SVM模型的参数是很难获悉任何信息的. 但是, SVM越来越受欢迎, 很多人发现SVM在真实世界的情况下表现得很好. (Oracle数据挖掘软件中有SVM, 但很多其他的算法都没有包含在该软件中.)  




-- k邻近  
k邻近(k nearest neighbors) 是解决分类问题的一种最简单的算法. 该算法的原理为:  
  1. 分析师构建训练的数据集.  
  2. 为了预测新数据的标签, 算法首先在训练数据集中寻找与新数据距离最近的k个观测值.  
  3. 新数据的标签标记为这k个邻居的投票结果.  


-- 分类树模型  
分类树(classification tree)模型与回归树几乎一样. 二者主要有两个关键区别. 第一, 分类回归树用不同的误差函数来度量不同的分割训练数据方式, 具体来说, 即使用Gini系数来度量模型的复杂度. 第二, 分类回归树用不同的方法来选择预测值. 叶子结点的预测值是通过测试数据的响应变量值投票来选取的.    

-- 神经网络  
神经网络可以解决回归和分类问题.  

-- 支持向量机  
SVM同样能够处理分类和回归的问题.  

-- 随机森林  
随机森林是另外一种解决回归或者分类问题的算法.  




-- 聚类  
数据挖掘技术中还有一个重要的算法是聚类. 聚类是从数据集中挖掘相似观测值集合的方法. 一组相似的观测值称为簇(cluster). R中有许多函数可以实现聚类.  


距离度量  
为了有效利用聚类算法, 首先需要度量观测值间的距离. 在R中常通过stats包里面的dist函数来实现. 
dist函数计算对象(矩阵或数据框)中两两间的距离, 返回的是距离矩阵(dist类对象). 

聚类算法  
k-means聚类是最简单的聚类算法之一. R中可以通过stats包里面的kmeans函数实现k-means聚类.  

```  


## R语言与网站分析  

```  

第5章  时间序列分析  
  时间序列  
  增长率  
  移动平均  
  指数平滑  
  ARIMA模型  
  

第6章  连续指标建模:回归分析  
  一元线性回归  
  多元回归分析  
  Logic回归分析  
  回归树CART  
  


第7章  分类指标建模:分类分析  
  决策树分类分析  
  贝叶斯分类  
  支持向量机SVM  
  人工神经网络  
  分类器的性能评估  

  
第8章  样本细分  
  数据降维  
  聚类分析  
  样本判别 knn(k最近邻分类) 基于knn算法的商品推荐系统    
  



```  

```  
----- 第5章  

时间序列对象是一种专为时间序列分析而设计的对象类型, 其中不仅包含指标的数值, 还包括时间轴信息.  
在R语言中, ts(<向量对象>)可以把一个向量转化为一个时间序列对象.  

环比增长率主要用于描述指标data的变化趋势, 其定义式为:  
ring.growth[t] = (data[t] - data[t-1]) / data[t-1]  

如果想避免周期性变化给数据分析带来的影响, 可以使用同比增长率来进行分析. 其定义格式如下:  
same.per.growth[t] = (date[t] - data(t-T)) / data(t-T)  
其中T是同比周期, 即data[t]和data[t-T]分别是前后两个周期中对应的指标数据.  


移动平均是从"信号与分析系统学"中发展出来的一种典型滤波方法. 其思想是使用近期的样本数据来预测当前和未来的数据. 它可以把抖动的曲线平滑化, 从而弱化随机变化和短期周期性变化的影响, 进而更明显地显示出长期的变化趋势曲线.  
简单/加权移动平均虽然可以过滤微小的随机变化, 突出长期变化趋势. 但是如果使用它进行预测, 其对指标实际的变化(周期性或长期趋势下的变化)并不敏感, 整体上会做相应的平移, 而这会导致预测值的滞后性.  


指数平滑预测的数值不仅可以体现指标的长期变化趋势trend, 还能体现其周期性seasonality变化趋势. 根据平滑次数的不同, 指数平滑又分为一次指数平滑, 二次指数平滑和三次指数平滑. 其中, 一次指数平滑可以建立平滑数据s[t], 并结合使用实际数据x[t]对指标x进行预测. 二次指数平滑通过对平滑数据s[t]的差分数据 s[t]-s[t-1] 进行再次平滑, 建立二次平滑数据b[t], 并结合一次平滑的预测值来对指标x进行预测, 其中b[t] 可以代表指标x的长期趋势. 三次指数平滑在二次指数平滑的基础上对 x[t]-s[t] 或 x[t]/s[t] 进行三次指数平滑,  并建立平滑数据p[t], 然后结合二次平滑的预测值来对指标x进行预测, 其中p[t]可以代表指标x的周期性变化趋势.   

ARIMA模型是时下较为流行的时间序列分析模型, 它通过对白噪声的组合来建立模型, 并保证模型和实际数据的残差也是一个均值很小的正态分布数据. 



----- 第6章  

数据指标的建模指的是, 使用若干自变量并建立公式, 以预测目标变量(因变量).  
如果研究的目标变量是连续型的, 则称其为回归分析(本章); 如果是分类型, 则称其为分类分析(下一章).  

回归分析的最基础的情况: 一元线性回归分析. 它规定模型f函数只能是 y = kx + b 的形式, 即只使用一个变量x(故称为一元)的线性形式来预测目标变量y.  
一元线性回归分析的原理  
  1. 最小二乘法  
  2. 样本方差和协方差  
  3. 计算模型的参数k和b  
  4. 衡量相关程度  
  5. 回归系数的显著性检验
     T检验, F检验  
  6. 模型误差(残差)  
  7. 预测  
  
多元线性回归分析建模与一元回归分析类似.  


理论上, 回归分析是在目标变量为连续型数据的情况下建模的, 它并不能处理目标变量为分类型数据的情况. 而login回归分析的思路是吧 "是否问题" 转化为 "是的概率" (连续变量), 进而使用回归分析的方法间接地研究分类分析的问题.    

回归分析是使用最小二乘法(旨在使获得的模型能最大限度地拟合样本数据)来估算模型参数的, login分析则使用最大似然法来估算. 其基本思想是: 当从总体中随机抽取n个样本后, 最合理的参数估计量应该使得这n个样本观测值的概率最大.    

回归树(也称分类回归树CART)主要以一种树状结构来表达回归分析模型的回归算法, 该类方法不仅可以应用于回归分析(称为回归树), 也可用于分类分析(称为分类树).  
R语言实现CART算法的核心函数是rpart包的epart函数.  
把继续做分支的节点叫做决策节点(non-leaf node), 不再进行分支的节点叫叶节点(leaf-node).  
一般来说, 随着拆分的增多(nsplit增加), 复杂性参量(cp)会单调下降(纯度越来越高), 但是预测误差(xerror)则会先降后升, 一般允许误差在一个标准差(xstd)内波动. 这就产生了在剪枝理论中比较著名的规则: 1-标准差规则. 其意思是: 首先要保证预测误差尽量小, 但不一定要去最小值, 而是允许它在 "最小的误差 +- 一个相应标准差" 的范围内.  




----- 第7章  

决策树算法的输出通常类似于一套由if-else(二分支)或case(多分支)语句组成的判定式, 因其画出的形状很像一棵树, 故把此类算法统称为决策树算法, 常用的决策树算法有C4.5, CART等.  
决策树的建立过程实际就是节点的分裂(split)过程.  
由于存在过分拟合等问题, 所以往往要对模型进行剪枝. 剪枝处理后, 虽然训练集的错误率会有所增加, 但是却能让新样本的预测错误率下降. 
  前剪枝preprune: 在模型建立过程中就完成剪枝, 即加强停止分裂的条件.  
  后剪枝postprune: 在模型建立之后才进行剪枝. 在剪枝前要设置停止剪枝的相应参数, 然后从模型的叶节点逐层向上剪枝.  
  
ID3算法是分类分析中的鼻祖级算法, 它以信息论中的信息增益为核心思想来建立树模型. C4.5算法实际上就是在ID3算法的基础上优化而成的.  
C4.5算法在另一个数据挖掘开源软件weka已经有很成熟的实现, 在R语言中, 可以通过RWeka包中的J48函数来调用weka的C4.5算法.  

CART的全称是Classification And Regression Tree, 即分类回归树. 它即可以对分类型目标变量建模(分类分析), 也可以对连续型目标变量建模(回归分析).  
与C4.5算法类似, CART算法也使用目标变量的纯度来分裂决策节点, 只是它使用的分裂度量是Gini增益. 需要注意的是, CART内部只支持二分支树.  


  
 
  







```  










  